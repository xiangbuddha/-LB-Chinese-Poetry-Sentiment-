# -LB-Chinese-Poetry-Sentiment-
This is a sentiment analysis model fine-tuned from 'qixun/bert-chinese-poem' for analyzing sentiments in traditional Chinese poems. The model can identify 7 different categories.
language: zh tags:

BERT
sentiment-analysis
chinese-poetry
text-classification
classical-chinese license: gpl-3.0 metrics:
accuracy
f1
LB Chinese Poetry Sentiment Analysis BERT
æ°¸è¿œæ€€å¿µæˆ‘æœ€äº²çˆ±çš„å°çŒ«è“å®ï¼Œæˆ‘ä»¬éƒ½çˆ±ä½ 

ğŸ“– æ¨¡å‹ç®€ä»‹ Model Description
è¿™æ˜¯ä¸€ä¸ªåŸºäº 'qixun/bert-chinese-poem' å¾®è°ƒçš„ä¸­æ–‡å¤è¯—æƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼Œä¸“é—¨ç”¨äºåˆ†æå¤å…¸è¯—è¯ä¸­çš„æƒ…æ„Ÿå€¾å‘ã€‚æ¨¡å‹å¯ä»¥è¯†åˆ«7ç§ä¸åŒçš„æƒ…æ„Ÿç±»åˆ«ï¼Œå¸®åŠ©ç†è§£è¯—è¯çš„æƒ…æ„Ÿå†…æ¶µã€‚ æ¨¡å‹å¯ä»¥è¯†åˆ«ä»¥ä¸‹7ç§æƒ…æ„Ÿ:ç§¯æ Positive æ¶ˆæ Negative å¹³é™ Calm æ•¬ç• Awe æ–°å¥‡ æ€å¿µ Nostalgia å‘å¾€ Aspiration

This is a sentiment analysis model fine-tuned from 'qixun/bert-chinese-poem' for analyzing sentiments in traditional Chinese poems. The model can identify 7 different categories.

ğŸš€ å¿«é€Ÿå¼€å§‹ Quick Start
å®‰è£…ä¾èµ– Installation
pip install transformers torch


1.åŸºç¡€ä½¿ç”¨ Basic Usage
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import torch.nn.functional as F


model_name = "Buddhafate/LB-Chinese-Poetry-Sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# ä¸­æ–‡æƒ…æ„Ÿæ ‡ç­¾ 
emotion_labels = ["ç§¯æ", "æ¶ˆæ", "å¹³é™", "æ•¬ç•", "æ–°å¥‡", "æ€å¿µ", "å‘å¾€"]

# å®Œæ•´åˆ†æå‡½æ•° 
def analyze_poem_sentiment(poem):
    """åˆ†æä¸­æ–‡è¯—è¯çš„ä¸»è¦æƒ…æ„Ÿç±»åˆ«åŠæ¦‚ç‡åˆ†å¸ƒ"""
    inputs = tokenizer(poem, return_tensors="pt", padding=True, truncation=True, max_length=128)
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probs = F.softmax(logits, dim=1)

    predicted_idx = torch.argmax(probs, dim=1).item()
    confidence = probs[0][predicted_idx].item()
    return {
        "åŸè¯—": poem,
        "ä¸»è¦æƒ…æ„Ÿ": emotion_labels[predicted_idx],
        "ç½®ä¿¡åº¦": confidence,
        "æƒ…æ„Ÿåˆ†å¸ƒ": {
            emotion_labels[i]: float(prob)
            for i, prob in enumerate(probs[0])
        }
    }

# ç¤ºä¾‹è¯—å¥ 
poem = """åº­å‰ç—…æ¡§è‡ªè§ç–ï¼Œé—¨å¤–æƒŠé¸¥ä¸å¯å‘¼ã€‚
é¥±å¬æ±Ÿå£°åå¹´äº‹ï¼Œæ¥å¯»é™ˆè¿¹ä¸€ç¯‡æ— ã€‚
æŠ•è’åæƒœäººå°†è€ï¼Œæœ›é²ç©ºå—Ÿé“å·³å­¤ã€‚
èµ–æœ‰èƒœå¤©åšå¿µåœ¨ï¼Œç¨åˆ†è‚èƒ†ä¸ææ¢§ã€‚"""

# è¾“å‡º 
result = analyze_poem_sentiment(poem)

print("=" * 60)
print(" è¯—è¯æƒ…æ„Ÿåˆ†æ")
print("=" * 60)
print(f"\nğŸ“œ åŸè¯—ï¼š\n{result['åŸè¯—']}\n")
print(f"ä¸»è¦æƒ…æ„Ÿï¼š{result['ä¸»è¦æƒ…æ„Ÿ']}  (ç½®ä¿¡åº¦: {result['ç½®ä¿¡åº¦']:.2%})")

print("\næƒ…æ„Ÿæ¦‚ç‡åˆ†å¸ƒï¼š")
for label, prob in sorted(result["æƒ…æ„Ÿåˆ†å¸ƒ"].items(), key=lambda x: x[1], reverse=True):
    bar = "â–ˆ" * int(prob * 20)
    print(f"  {label:>4s}: {bar:<20s} {prob:>6.2%}")





2.æ‰¹é‡å¤„ç† Batch Processing

def batch_analyze(poems):
    """
    å¤šé¦–è¯—è¯
    """

    if isinstance(poems, str):
        poems = [poems]
    elif not isinstance(poems, list):
        raise ValueError("è¾“å…¥å¿…é¡»æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨")

    inputs = tokenizer(poems, return_tensors="pt", padding=True, truncation=True, max_length=128)
    

    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.softmax(outputs.logits, dim=-1)
        predicted_classes = torch.argmax(probs, dim=-1)
 
    results = []
    for i, (poem, pred_idx) in enumerate(zip(poems, predicted_classes)):
        pred_idx = pred_idx.item()
        confidence = probs[i][pred_idx].item()
        all_probs = {
            emotion_labels[j]: probs[i][j].item()
            for j in range(len(emotion_labels))
        }
        results.append({
            "åŸè¯—": poem,
            "ä¸»è¦æƒ…æ„Ÿ": emotion_labels[pred_idx],
            "ç½®ä¿¡åº¦": confidence,
            "æƒ…æ„Ÿåˆ†å¸ƒ": all_probs
        })

    return results



æ€§èƒ½æŒ‡æ ‡ Performance Metrics
  åˆ†ç±»æŠ¥å‘Š Classification Report
  æ¨¡å‹åœ¨æµ‹è¯•é›†ï¼ˆ1,609é¦–è¯—è¯ï¼‰ä¸Šçš„è¡¨ç°ï¼š
  æƒ…æ„Ÿç±»åˆ«PrecisionRecallF1-ScoreSupportç§¯æ0.960.980.97297æ¶ˆæ0.980.950.97244å¹³é™0.980.940.96249æ•¬ç•0.990.940.97212æ–°å¥‡0.970.940.95250æ€å¿µ0.960.840.8961å‘å¾€0.980.900.93296
  æ€»ä½“æŒ‡æ ‡ Overall Metrics

  Macro Average å®å¹³å‡

  Precision: 0.97
  Recall: 0.93
  F1-Score: 0.95



æ€§èƒ½åˆ†æ Performance Analysis

  æœ€ä½³è¡¨ç°ç±»åˆ«ï¼š

  æ•¬ç• (F1: 0.97, Precision: 0.99)
  ç§¯æ (F1: 0.97, Recall: 0.98)
  æ¶ˆæ (F1: 0.97, Precision: 0.98)


éœ€è¦æ³¨æ„çš„ç±»åˆ«ï¼š

  æ€å¿µç±»åˆ«çš„æ ·æœ¬è¾ƒå°‘ï¼ˆä»…61ä¸ªï¼‰ï¼ŒF1åˆ†æ•°ç›¸å¯¹è¾ƒä½ï¼ˆ0.89ï¼‰
  ä½†å…¶Precisionä»è¾¾åˆ°0.96ï¼Œè¯´æ˜æ¨¡å‹åˆ¤æ–­ä¸º"æ€å¿µ"æ—¶å‡†ç¡®ç‡å¾ˆé«˜


æ•´ä½“è¡¨ç°ï¼š

  å®å¹³å‡F1åˆ†æ•°è¾¾åˆ°0.95ï¼Œè¡¨æ˜æ¨¡å‹åœ¨å„ç±»åˆ«ä¸Šè¡¨ç°å‡è¡¡
  é«˜Precisionï¼ˆ0.97ï¼‰è¯´æ˜æ¨¡å‹çš„é¢„æµ‹ç»“æœå¯ä¿¡åº¦é«˜



æŠ€æœ¯ç»†èŠ‚ Technical Details

  åŸºç¡€æ¨¡å‹ Base Model: qixun/bert-chinese-poem
  æ¨¡å‹æ¶æ„ Architecture: BERT for Sequence Classification
  è®­ç»ƒæ ·æœ¬ Training Samples: ~10,000 å¤è¯—è¯
  æœ€å¤§é•¿åº¦ Max Length: 512 tokens
  è®­ç»ƒè½®æ•° Epochs: 5
  æ‰¹æ¬¡å¤§å° Batch Size: 24
  å­¦ä¹ ç‡ Learning Rate: 2e-5
  ä¼˜åŒ–å™¨ Optimizer: AdamW
  è®­ç»ƒè®¾å¤‡: GPU with CUDA


ä½¿ç”¨é™åˆ¶ Limitations

  æ¨¡å‹ä¸»è¦é’ˆå¯¹å¤å…¸è¯—è¯è®­ç»ƒï¼Œå¯¹ç°ä»£è¯—æ­Œæ•ˆæœå¯èƒ½æœ‰é™
  è¾“å…¥æ–‡æœ¬å»ºè®®ä¸è¶…è¿‡512 ä¸ªtoken
  å¯¹äºå«æœ‰å¤æ‚éšå–»çš„è¯—å¥ï¼Œå¯èƒ½éœ€è¦ç»“åˆä¸Šä¸‹æ–‡ç†è§£
  ç”±äºç»§æ‰¿è‡ªGPL-3.0è®¸å¯è¯ï¼Œä½¿ç”¨æœ¬æ¨¡å‹çš„é¡¹ç›®ä¹Ÿéœ€è¦å¼€æº



è‡´è°¢ Acknowledgments

åŸºç¡€æ¨¡å‹æä¾›è€…: qixun/bert-chinese-poem
è®­ç»ƒæ¡†æ¶: Hugging Face Transformers
ç‰¹åˆ«çºªå¿µ: æˆ‘çš„çˆ±çŒ«è“å®å¼Ÿå¼Ÿ (2016-2025)

ğŸ“„ è®¸å¯è¯ License
æœ¬æ¨¡å‹ç»§æ‰¿è‡ªåŸå§‹æ¨¡å‹çš„ GPL-3.0 è®¸å¯è¯ã€‚ä½¿ç”¨æœ¬æ¨¡å‹çš„é¡¹ç›®éœ€è¦éµå®ˆç›¸åŒçš„å¼€æºåè®®ã€‚
This model inherits the GPL-3.0 license from the original model. Projects using this model must comply with the same open-source license.
ğŸ“® è”ç³»æ–¹å¼ Contact
emailï¼šx130319@gmail.com  WeChat:NarayanaJupiter
